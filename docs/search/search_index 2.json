{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Databus Databus is an async middleware platform. It can pull, queue, process and push data within the defined rules; and is managed over a simple Web interface . It is open source and available at GitHub & PYPI Used by It is currently used by Eczacibasi Tuketim to accept various B2B documents over E-Mail and transfer them to SAP. Databus is also the backbone of various products; such as Measy . You can inspect Measy as an implementation demo of Databus. Contact Databus is written & maintained by Dr. Kerem Koseoglu .","title":"Home"},{"location":"#databus","text":"Databus is an async middleware platform. It can pull, queue, process and push data within the defined rules; and is managed over a simple Web interface . It is open source and available at GitHub & PYPI","title":"Databus"},{"location":"#used-by","text":"It is currently used by Eczacibasi Tuketim to accept various B2B documents over E-Mail and transfer them to SAP. Databus is also the backbone of various products; such as Measy . You can inspect Measy as an implementation demo of Databus.","title":"Used by"},{"location":"#contact","text":"Databus is written & maintained by Dr. Kerem Koseoglu .","title":"Contact"},{"location":"architecture/","text":"Architecture","title":"Architecture"},{"location":"architecture/#architecture","text":"","title":"Architecture"},{"location":"client/","text":"Client Defines a client company using Databus. Each client can has its own reader / processor / pusher classes. A singular Databus instance can support multiple clients. Client To add a new client (called abc); json_db : Create the client folder structure under /data/clients/ as described in database section. sql_db : Fill the SQL Server tables just like the 'demo' client If a client doesn't have any users defined, this means that the authentication is not active. The web interface will allow any login for that client. User You may define two user roles, which will affect their capabilities in the web interface. operator : The standard user, able to browse through files, but unable to configure anything. administrator : The power user, able to configure as well. There is also a special God-Mode user. Client: root, user: root. This user can perform actions that no other user can, such as shutting down the system. Check the web section for more details. Client Passenger Describes a particular passenger definition of a client. Each client can contain multiple passengers; as long as they are defined in the database properly.","title":"Client"},{"location":"client/#client","text":"Defines a client company using Databus. Each client can has its own reader / processor / pusher classes. A singular Databus instance can support multiple clients.","title":"Client"},{"location":"client/#client_1","text":"To add a new client (called abc); json_db : Create the client folder structure under /data/clients/ as described in database section. sql_db : Fill the SQL Server tables just like the 'demo' client If a client doesn't have any users defined, this means that the authentication is not active. The web interface will allow any login for that client.","title":"Client"},{"location":"client/#user","text":"You may define two user roles, which will affect their capabilities in the web interface. operator : The standard user, able to browse through files, but unable to configure anything. administrator : The power user, able to configure as well. There is also a special God-Mode user. Client: root, user: root. This user can perform actions that no other user can, such as shutting down the system. Check the web section for more details.","title":"User"},{"location":"client/#client-passenger","text":"Describes a particular passenger definition of a client. Each client can contain multiple passengers; as long as they are defined in the database properly.","title":"Client Passenger"},{"location":"database/","text":"Database Databus needs a database to store client and queue data. Out of the box, it supports: json_db : A custom written engine which stores the data on the disk as JSON files. sql_db : An engine to store data on a Microsoft SQL Server instance. Both database engines have the same functionality. But naturally; SQL Server will be faster if you are dealing with large volumes of data. Databus has the ability to export data. Check the export menu on the Web interface. JSON DB JSON DB is a simple but robust solution to store queue data. It is based on storing data on the disk in JSON format. It is obviously not advisable for large volumes of data. A typical JSON DB implementation will have the following structure: /data/json_db/clients : root directory /data/json_db/clients/demo : Demo client /data/json_db/clients/demo/config.json : Configuration of demo client /data/json_db/clients/demo/log : Log files (initially empty). For each new log, Databus will put a new .json file here. /data/json_db/clients/demo/pqueue : Queue files (initially empty). For each new queue entry, Databus will open a new folder here. For each new client you want to add, you can imitate the folder structure of the demo client. config.json will have the following format: { \"log_life_span\": 1, \"passengers\": [ { \"name\": \"databus.passenger.demo.demo_passenger_1\", \"processors\": [ \"databus.processor.demo.demo_processor_1\" ], \"pullers\": [ \"databus.puller.demo.demo_puller_1\" ], \"pushers\": [ \"databus.pusher.demo.demo_pusher_1\" ], \"queue\": \"databus.pqueue.primal_queue\", \"queue_life_span\": 1, \"sync_frequency\": 1 } ], \"users\": [ { \"password\": \"demo\", \"role\": \"administrator\", \"token\": \"267c598e-f994-11ea-9005-acde48001122\", \"username\": \"demo\" } ] } log_life_span : For how many days should the log files live on the disk? They will be deleted afterwards. passengers : List of passengers of the client. name : Name of the Python passenger module. This is our data to transport. pullers : Names of Python puller modules. Those are our data sources. processors : Names of Python processor modules. Those are the code files doing filtering, validation, etc. pushers : Names of Python pusher modules. Those are our data targets. queue : Name of the Python queue module. Typically, you'll be using the primal queue. queue_life_span : For how many days should the completed queue files live on the disk? They will be deleted afterwards. sync_frequency : How often (in minutes) should Databus sync this passenger? users : List of Databus users of this client. Sub-fields are intuitive. username : Obvious password : Obvious role : Either \"administrator\" or \"operator\". Administrator has more capabilities on the web interface. SQL Server Databus can run on a traditional SQL Server instance. To create a brand new set of Databus tables, you can use the file /databus/database/sql_db/creation_script.sql . After the database is created, you can start Databus with the appropriate dispatcher ticket to start against the database. The tables are very intuitive. Following the explanation under JSON DB, just browse through the tables and you'll find your way around. Custom database You can implement your own database engine too. All you need to do is to derive a new class from databus.database.abstract_database. Remember passing your own module name to the dispatcher as seen above.","title":"Database"},{"location":"database/#database","text":"Databus needs a database to store client and queue data. Out of the box, it supports: json_db : A custom written engine which stores the data on the disk as JSON files. sql_db : An engine to store data on a Microsoft SQL Server instance. Both database engines have the same functionality. But naturally; SQL Server will be faster if you are dealing with large volumes of data. Databus has the ability to export data. Check the export menu on the Web interface.","title":"Database"},{"location":"database/#json-db","text":"JSON DB is a simple but robust solution to store queue data. It is based on storing data on the disk in JSON format. It is obviously not advisable for large volumes of data. A typical JSON DB implementation will have the following structure: /data/json_db/clients : root directory /data/json_db/clients/demo : Demo client /data/json_db/clients/demo/config.json : Configuration of demo client /data/json_db/clients/demo/log : Log files (initially empty). For each new log, Databus will put a new .json file here. /data/json_db/clients/demo/pqueue : Queue files (initially empty). For each new queue entry, Databus will open a new folder here. For each new client you want to add, you can imitate the folder structure of the demo client. config.json will have the following format: { \"log_life_span\": 1, \"passengers\": [ { \"name\": \"databus.passenger.demo.demo_passenger_1\", \"processors\": [ \"databus.processor.demo.demo_processor_1\" ], \"pullers\": [ \"databus.puller.demo.demo_puller_1\" ], \"pushers\": [ \"databus.pusher.demo.demo_pusher_1\" ], \"queue\": \"databus.pqueue.primal_queue\", \"queue_life_span\": 1, \"sync_frequency\": 1 } ], \"users\": [ { \"password\": \"demo\", \"role\": \"administrator\", \"token\": \"267c598e-f994-11ea-9005-acde48001122\", \"username\": \"demo\" } ] } log_life_span : For how many days should the log files live on the disk? They will be deleted afterwards. passengers : List of passengers of the client. name : Name of the Python passenger module. This is our data to transport. pullers : Names of Python puller modules. Those are our data sources. processors : Names of Python processor modules. Those are the code files doing filtering, validation, etc. pushers : Names of Python pusher modules. Those are our data targets. queue : Name of the Python queue module. Typically, you'll be using the primal queue. queue_life_span : For how many days should the completed queue files live on the disk? They will be deleted afterwards. sync_frequency : How often (in minutes) should Databus sync this passenger? users : List of Databus users of this client. Sub-fields are intuitive. username : Obvious password : Obvious role : Either \"administrator\" or \"operator\". Administrator has more capabilities on the web interface.","title":"JSON DB"},{"location":"database/#sql-server","text":"Databus can run on a traditional SQL Server instance. To create a brand new set of Databus tables, you can use the file /databus/database/sql_db/creation_script.sql . After the database is created, you can start Databus with the appropriate dispatcher ticket to start against the database. The tables are very intuitive. Following the explanation under JSON DB, just browse through the tables and you'll find your way around.","title":"SQL Server"},{"location":"database/#custom-database","text":"You can implement your own database engine too. All you need to do is to derive a new class from databus.database.abstract_database. Remember passing your own module name to the dispatcher as seen above.","title":"Custom database"},{"location":"dispatcher/","text":"Dispatcher This is the main engine of Databus. It is responsible of scheduling & invoking drivers ; which are responsible of actually transporting data between systems. Typically, your main access point to databus is the dispatcher. When you start up the dispatcher, it means that Databus is up and running. Check startup for alternative ways of starting Databus. Dispatcher ticket To start the dispatcher, you might probably need to provide a dispatcher ticket from your app to Databus. If you don't, it will start with the default options; which might be good enough for many cases! Structure of a dispatcher ticket can be seen in databus.dispatcher.abstract_dispatcher.DispatcherTicket . Parameters that should be left empty for the default values in most cases: p_database_factory : Name of the database factory module p_driver_factory : Name of the driver factory module p_passenger_factory : Name of the passenger factory module p_queue_factory : Name of the queue factory module p_puller_factory : Name of the puller factory module p_processor_factory : Name of the processor factory module p_pusher_factory : Name of the pusher factory module p_driver_module : Name of the driver module Parameters which are advised to be filled are: p_database_module : Database engine module to be used. The default engine is JSON DB ; however, you can use any supplied or custom-developed database engine here. p_database_arguments : Arguments of the provided database module. JSON DB doesn't need any arguments, SQL Server needs some arguments, and your custom-developed database might need some arguments. p_dispatcher_observer : If you have implemented a custom observer class (explained above), you can pass your observer object here. p_run_web_server : Obvious p_web_server_port : Obvious p_external_config_files : If you have additional local JSON configuration files, you can pass their paths here so they become editable through the web interface . p_system_alias : This value is displayed on top of the web interface . Useful to distinguish test - live systems. Inspect databus.dispatcher.abstract_dispatcher for further startup options which might have been missed in the documentation. Observer \"Observer\" is a useful design pattern ; which also took a place within the dispatcher. If your app needs to be aware of significant events within the dispatcher, you can write a custom class implementing databus.dispatcher.observer.DispatcherObserver, and pass the observer object to the dispatcher ticket (as explained below). Check databus/dispatcher/observer.py for a list of events you can be listening to.","title":"Dispatcher"},{"location":"dispatcher/#dispatcher","text":"This is the main engine of Databus. It is responsible of scheduling & invoking drivers ; which are responsible of actually transporting data between systems. Typically, your main access point to databus is the dispatcher. When you start up the dispatcher, it means that Databus is up and running. Check startup for alternative ways of starting Databus.","title":"Dispatcher"},{"location":"dispatcher/#dispatcher-ticket","text":"To start the dispatcher, you might probably need to provide a dispatcher ticket from your app to Databus. If you don't, it will start with the default options; which might be good enough for many cases! Structure of a dispatcher ticket can be seen in databus.dispatcher.abstract_dispatcher.DispatcherTicket . Parameters that should be left empty for the default values in most cases: p_database_factory : Name of the database factory module p_driver_factory : Name of the driver factory module p_passenger_factory : Name of the passenger factory module p_queue_factory : Name of the queue factory module p_puller_factory : Name of the puller factory module p_processor_factory : Name of the processor factory module p_pusher_factory : Name of the pusher factory module p_driver_module : Name of the driver module Parameters which are advised to be filled are: p_database_module : Database engine module to be used. The default engine is JSON DB ; however, you can use any supplied or custom-developed database engine here. p_database_arguments : Arguments of the provided database module. JSON DB doesn't need any arguments, SQL Server needs some arguments, and your custom-developed database might need some arguments. p_dispatcher_observer : If you have implemented a custom observer class (explained above), you can pass your observer object here. p_run_web_server : Obvious p_web_server_port : Obvious p_external_config_files : If you have additional local JSON configuration files, you can pass their paths here so they become editable through the web interface . p_system_alias : This value is displayed on top of the web interface . Useful to distinguish test - live systems. Inspect databus.dispatcher.abstract_dispatcher for further startup options which might have been missed in the documentation.","title":"Dispatcher ticket"},{"location":"dispatcher/#observer","text":"\"Observer\" is a useful design pattern ; which also took a place within the dispatcher. If your app needs to be aware of significant events within the dispatcher, you can write a custom class implementing databus.dispatcher.observer.DispatcherObserver, and pass the observer object to the dispatcher ticket (as explained below). Check databus/dispatcher/observer.py for a list of events you can be listening to.","title":"Observer"},{"location":"driver/","text":"Driver Driver is the component responsible of running a data transfer operation. Reminder: Dispatcher is responsible of scheduling drivers. When the time comes, driver is the component doing the actual work. When the driver is executed; it will take the following steps; as seen in databus/driver/primal_driver.py: Pull new passengers from the source system Seat passengers into the queue Notify the source system about seated passengers Process seated passengers Push seated passengers to the target system The queue takes note of failures of each step. Driver will retry any failed step on the next schedule. However, users are able to manually alter statuses of each step through the web interface.","title":"Driver"},{"location":"driver/#driver","text":"Driver is the component responsible of running a data transfer operation. Reminder: Dispatcher is responsible of scheduling drivers. When the time comes, driver is the component doing the actual work. When the driver is executed; it will take the following steps; as seen in databus/driver/primal_driver.py: Pull new passengers from the source system Seat passengers into the queue Notify the source system about seated passengers Process seated passengers Push seated passengers to the target system The queue takes note of failures of each step. Driver will retry any failed step on the next schedule. However, users are able to manually alter statuses of each step through the web interface.","title":"Driver"},{"location":"how/","text":"How it works A simple flow Once Databus is up and running, here is what it does: Passengers ; such as E-Mails or customer orders, are stored in the source system(s). Puller(s) detect & read new passengers periodically. Queue stores those passengers in the database . Processors runs any required operations on the passengers; such as filtering, data conversion, etc. Pusher(s) transport the passenger to the target system(s). Significant features A simple web interface Supports any number of distinct clients with different configurations Each client can transport any number of passengers Different passengers can have different sync schedules In case of an error, Databus will retry the operation You can pause / resume Databus if needed","title":"How it works"},{"location":"how/#how-it-works","text":"","title":"How it works"},{"location":"how/#a-simple-flow","text":"Once Databus is up and running, here is what it does: Passengers ; such as E-Mails or customer orders, are stored in the source system(s). Puller(s) detect & read new passengers periodically. Queue stores those passengers in the database . Processors runs any required operations on the passengers; such as filtering, data conversion, etc. Pusher(s) transport the passenger to the target system(s).","title":"A simple flow"},{"location":"how/#significant-features","text":"A simple web interface Supports any number of distinct clients with different configurations Each client can transport any number of passengers Different passengers can have different sync schedules In case of an error, Databus will retry the operation You can pause / resume Databus if needed","title":"Significant features"},{"location":"installation/","text":"Installation To install from PYPI: pip install databus-keremkoseoglu To install from GitHub: pip install git+http://github.com/keremkoseoglu/databus.git","title":"Installation"},{"location":"installation/#installation","text":"To install from PYPI: pip install databus-keremkoseoglu To install from GitHub: pip install git+http://github.com/keremkoseoglu/databus.git","title":"Installation"},{"location":"passenger/","text":"Passenger Passenger defines a data type to travel between systems. If you are pulling E-Mails and forwarding them to your target system, E-Mail is your passenger. General structure of a passenger can be seen under /databus/passenger/abstract_passenger.py . Each passenger implementation has to be a class derived from that. You can find some demo implementations under databus/passenger/demo, or check \" Used by \" to see real life examples. Properties A passenger will have the following properties: external_id : The unique ID given by the source system. internal_id : A unique ID given by Databus. source_system : Name of the system from which the passenger has been pulled. attachments : A list of attachments (see below for details). puller_module : Name of the Python module which pulled the passenger. pull_datetime : The date + time on which the passenger was pulled. log_guids : List of log ID's linked to this passenger Attachments Except the properties mentioned above, any data that needs to be stored with the passenger can be added as an attachment. Databus supports text and binary attachments. For example; if you got an E-Mail message, each attachment can obviously be added as a passenger attachment. But if you need to store the mail body as well, you can add it as an artificial attachment called _body.html . Or, if you need to store additional properties with the passenger, create an artificial attachment called _props.json and store it as an attachment. Each attachment of a given passenger must have a unique name. Supplied passengers Currently, Databus provides a passenger class for E-Mail messages under databus/passenger/email.py . If you need to pull E-Mails from a mail server, you can use this passenger. Hint: It also has an Exchange Server puller to help you with that. Implementing a new passenger Create a new class derived from databus.passenger.abstract_passenger Ensure that your .py file has only one class (which is the passenger) Ensure calling super(). init () You may need to implement corresponding puller / processor / pusher classes as well Add the puller class to your client configuration json_db : /data/json_db/clients/(client name)/config.json sql_db : databus.passenger","title":"Passenger"},{"location":"passenger/#passenger","text":"Passenger defines a data type to travel between systems. If you are pulling E-Mails and forwarding them to your target system, E-Mail is your passenger. General structure of a passenger can be seen under /databus/passenger/abstract_passenger.py . Each passenger implementation has to be a class derived from that. You can find some demo implementations under databus/passenger/demo, or check \" Used by \" to see real life examples.","title":"Passenger"},{"location":"passenger/#properties","text":"A passenger will have the following properties: external_id : The unique ID given by the source system. internal_id : A unique ID given by Databus. source_system : Name of the system from which the passenger has been pulled. attachments : A list of attachments (see below for details). puller_module : Name of the Python module which pulled the passenger. pull_datetime : The date + time on which the passenger was pulled. log_guids : List of log ID's linked to this passenger","title":"Properties"},{"location":"passenger/#attachments","text":"Except the properties mentioned above, any data that needs to be stored with the passenger can be added as an attachment. Databus supports text and binary attachments. For example; if you got an E-Mail message, each attachment can obviously be added as a passenger attachment. But if you need to store the mail body as well, you can add it as an artificial attachment called _body.html . Or, if you need to store additional properties with the passenger, create an artificial attachment called _props.json and store it as an attachment. Each attachment of a given passenger must have a unique name.","title":"Attachments"},{"location":"passenger/#supplied-passengers","text":"Currently, Databus provides a passenger class for E-Mail messages under databus/passenger/email.py . If you need to pull E-Mails from a mail server, you can use this passenger. Hint: It also has an Exchange Server puller to help you with that.","title":"Supplied passengers"},{"location":"passenger/#implementing-a-new-passenger","text":"Create a new class derived from databus.passenger.abstract_passenger Ensure that your .py file has only one class (which is the passenger) Ensure calling super(). init () You may need to implement corresponding puller / processor / pusher classes as well Add the puller class to your client configuration json_db : /data/json_db/clients/(client name)/config.json sql_db : databus.passenger","title":"Implementing a new passenger"},{"location":"processor/","text":"Processor A processor can do anything you want to do between a pull and push operation. Filtering data is a typical processor operation. You can find some demo implementations under databus/processor/demo, or check \" Used by \" to see real life examples. Supplied processors Currently, Databus provides a ready-to-use processor in databus/processor/email_filter_excel_attachment.py . This is meant to be used with the E-Mail passenger . It will scan the attachments of the E-Mail, and if it doesn't contain any Excel attachments, the passenger will be marked as \"complete\" and won't be processed. This is particularly useful if you are expecting E-Mails with Excel attachments. Implementing a new processor To implement a new processor; Ensure that the corresponding passenger & puller classes exist. If not, create them first. Derive a new class from databus.processor.abstract_processor Ensure that your .py file has only one class (which is the processor) Ensure calling super(). init () Add the processor class to your client configuration. json_db : /data/json_db/clients/(client name)/config.json sql_db : databus.processor","title":"Processor"},{"location":"processor/#processor","text":"A processor can do anything you want to do between a pull and push operation. Filtering data is a typical processor operation. You can find some demo implementations under databus/processor/demo, or check \" Used by \" to see real life examples.","title":"Processor"},{"location":"processor/#supplied-processors","text":"Currently, Databus provides a ready-to-use processor in databus/processor/email_filter_excel_attachment.py . This is meant to be used with the E-Mail passenger . It will scan the attachments of the E-Mail, and if it doesn't contain any Excel attachments, the passenger will be marked as \"complete\" and won't be processed. This is particularly useful if you are expecting E-Mails with Excel attachments.","title":"Supplied processors"},{"location":"processor/#implementing-a-new-processor","text":"To implement a new processor; Ensure that the corresponding passenger & puller classes exist. If not, create them first. Derive a new class from databus.processor.abstract_processor Ensure that your .py file has only one class (which is the processor) Ensure calling super(). init () Add the processor class to your client configuration. json_db : /data/json_db/clients/(client name)/config.json sql_db : databus.processor","title":"Implementing a new processor"},{"location":"puller/","text":"Puller Puller classes are defined to fetch passengers from source systems. For each source system, you may implement your own puller class. Databus has a built-in abstract Exchange puller; which you can implement for your own purposes. You can find some demo implementations under databus/puller/demo, or check \" Used by \" to see real life examples. Supplied pullers Exchange Server Databus provides an abstract puller class for Exhange Server under databus/puller/abstract_exchange.py. If you need to get E-Mails from an Exchange Server, you can implement your concrete class and start using it right away. Exchange Server will return passengers of type E-Mail . You basically need to fill two points here: settings : Return your Exchange credentials and settings notify_passengers_seated : The action to take when a passenger is seated. Typically, you would want to call one of the ...seated_passengers... methods from here. A sample implementation can be found in Measy . Multi Exchange Server If you need to check multiple Exchange accounts for a particular type of E-Mail, you'll be pleased to know that Databus provides an abstract puller just for that! Checkdatabus/puller/abstract_multi_exchange.py. Create your own concrete class, fill the abstract methods and you are good to go! Obviously, it uses Exchange Server puller (explained above) behind the scenes. A sample implementation can be found in Measy . Implementing a new puller To implement a new puller; Ensure that the corresponding passenger class exists. If not, create your passenger class first. Derive a new class from databus.puller.abstract_puller Ensure that your .py file has only one class (which is the puller) Ensure calling super(). init () Add the puller class to your client configuration. json_db : /data/json_db/clients/(client name)/config.json sql_db : databus.puller","title":"Puller"},{"location":"puller/#puller","text":"Puller classes are defined to fetch passengers from source systems. For each source system, you may implement your own puller class. Databus has a built-in abstract Exchange puller; which you can implement for your own purposes. You can find some demo implementations under databus/puller/demo, or check \" Used by \" to see real life examples.","title":"Puller"},{"location":"puller/#supplied-pullers","text":"","title":"Supplied pullers"},{"location":"puller/#exchange-server","text":"Databus provides an abstract puller class for Exhange Server under databus/puller/abstract_exchange.py. If you need to get E-Mails from an Exchange Server, you can implement your concrete class and start using it right away. Exchange Server will return passengers of type E-Mail . You basically need to fill two points here: settings : Return your Exchange credentials and settings notify_passengers_seated : The action to take when a passenger is seated. Typically, you would want to call one of the ...seated_passengers... methods from here. A sample implementation can be found in Measy .","title":"Exchange Server"},{"location":"puller/#multi-exchange-server","text":"If you need to check multiple Exchange accounts for a particular type of E-Mail, you'll be pleased to know that Databus provides an abstract puller just for that! Checkdatabus/puller/abstract_multi_exchange.py. Create your own concrete class, fill the abstract methods and you are good to go! Obviously, it uses Exchange Server puller (explained above) behind the scenes. A sample implementation can be found in Measy .","title":"Multi Exchange Server"},{"location":"puller/#implementing-a-new-puller","text":"To implement a new puller; Ensure that the corresponding passenger class exists. If not, create your passenger class first. Derive a new class from databus.puller.abstract_puller Ensure that your .py file has only one class (which is the puller) Ensure calling super(). init () Add the puller class to your client configuration. json_db : /data/json_db/clients/(client name)/config.json sql_db : databus.puller","title":"Implementing a new puller"},{"location":"pusher/","text":"Pusher Pusher classes are defined to send passengers to target systems. For each target system, you may implement your own pusher class. You can find some demo implementations under databus/pusher/demo, or check \" Used by \" to see real life examples. Implementing a new pusher To implement a new pusher; Ensure that the corresponding passenger & puller classes exist. If not, create them first. Derive a new class from databus.pusher.abstract_pusher Ensure that your .py file has only one class (which is the pusher) Ensure calling super(). init () Add the pusher class to your client configuration. json_db : /data/json_db/clients/(client name)/config.json sql_db : databus.processor","title":"Pusher"},{"location":"pusher/#pusher","text":"Pusher classes are defined to send passengers to target systems. For each target system, you may implement your own pusher class. You can find some demo implementations under databus/pusher/demo, or check \" Used by \" to see real life examples.","title":"Pusher"},{"location":"pusher/#implementing-a-new-pusher","text":"To implement a new pusher; Ensure that the corresponding passenger & puller classes exist. If not, create them first. Derive a new class from databus.pusher.abstract_pusher Ensure that your .py file has only one class (which is the pusher) Ensure calling super(). init () Add the pusher class to your client configuration. json_db : /data/json_db/clients/(client name)/config.json sql_db : databus.processor","title":"Implementing a new pusher"},{"location":"queue/","text":"Queue Just what it says. When you pull a new passenger , it is stored in the queue until: The puller system is notified All processors are complete The passenger is pushed to the target system Therefore; the queue keeps track of the status of each puller , processor and pusher . It also keeps track of its corresponding log files, which can be viewed on the web interface . Even after everything is completed, the passenger lingers in the queue for a while. You can determine the wait time per client . Check database section for queue configuration details. Databus comes with a default queue implementation: databus/pqueue/primal_queue.py . You'll want to use this most of the time. Implementing a custom queue Although highly improbable; if, for whatever reason, you need to implement your own queue class; here are the steps necessary. Derive a new class from databus.pqueue.abstract_queue Ensure that your .py file has only one class (which is the queue) Ensure calling super(). init () Add the queue class to your client configuration. json_db : /data/json_db/clients/(client name)/config.json sql_db : databus.passenger","title":"Queue"},{"location":"queue/#queue","text":"Just what it says. When you pull a new passenger , it is stored in the queue until: The puller system is notified All processors are complete The passenger is pushed to the target system Therefore; the queue keeps track of the status of each puller , processor and pusher . It also keeps track of its corresponding log files, which can be viewed on the web interface . Even after everything is completed, the passenger lingers in the queue for a while. You can determine the wait time per client . Check database section for queue configuration details. Databus comes with a default queue implementation: databus/pqueue/primal_queue.py . You'll want to use this most of the time.","title":"Queue"},{"location":"queue/#implementing-a-custom-queue","text":"Although highly improbable; if, for whatever reason, you need to implement your own queue class; here are the steps necessary. Derive a new class from databus.pqueue.abstract_queue Ensure that your .py file has only one class (which is the queue) Ensure calling super(). init () Add the queue class to your client configuration. json_db : /data/json_db/clients/(client name)/config.json sql_db : databus.passenger","title":"Implementing a custom queue"},{"location":"startup/","text":"Startup Running a Databus instance is really easy! Default configuration from databus.dispatcher.primal_factory import PrimalDispatcherFactory PrimalDispatcherFactory().create_dispatcher().start() This will start Databus with the default configuration, which uses json_db. Visit http://127.0.0.1:5000 to see what it's been doing. The default demo account is demo:demo:demo. The default admin account is root:root:root. On a live system, you are advised to change this in your database . Custom configuration To start Databus with a custom configuration, you can provide a dispatcher ticket . Here is an example. from databus.database.sql_db.sql_database_arguments import SqlDatabaseArguments from databus.dispatcher.abstract_dispatcher import DispatcherTicket from databus.dispatcher.primal_factory import PrimalDispatcherFactory sql_args = { SqlDatabaseArguments.KEY_DATABASE: \"Master\", SqlDatabaseArguments.KEY_PASSWORD: \"reallyStrongPwd123\", SqlDatabaseArguments.KEY_SCHEMA: \"databus\", SqlDatabaseArguments.KEY_SERVER: \"127.0.0.1,1433\", SqlDatabaseArguments.KEY_USERNAME: \"SA\" } ticket = DispatcherTicket( p_database_module=\"databus.database.sql_db.sql_database\", p_database_arguments=sql_args ) PrimalDispatcherFactory().create_dispatcher(p_ticket=ticket).start() Check the dispatcher ticket section for a complete list of parameters.","title":"Startup"},{"location":"startup/#startup","text":"Running a Databus instance is really easy!","title":"Startup"},{"location":"startup/#default-configuration","text":"from databus.dispatcher.primal_factory import PrimalDispatcherFactory PrimalDispatcherFactory().create_dispatcher().start() This will start Databus with the default configuration, which uses json_db. Visit http://127.0.0.1:5000 to see what it's been doing. The default demo account is demo:demo:demo. The default admin account is root:root:root. On a live system, you are advised to change this in your database .","title":"Default configuration"},{"location":"startup/#custom-configuration","text":"To start Databus with a custom configuration, you can provide a dispatcher ticket . Here is an example. from databus.database.sql_db.sql_database_arguments import SqlDatabaseArguments from databus.dispatcher.abstract_dispatcher import DispatcherTicket from databus.dispatcher.primal_factory import PrimalDispatcherFactory sql_args = { SqlDatabaseArguments.KEY_DATABASE: \"Master\", SqlDatabaseArguments.KEY_PASSWORD: \"reallyStrongPwd123\", SqlDatabaseArguments.KEY_SCHEMA: \"databus\", SqlDatabaseArguments.KEY_SERVER: \"127.0.0.1,1433\", SqlDatabaseArguments.KEY_USERNAME: \"SA\" } ticket = DispatcherTicket( p_database_module=\"databus.database.sql_db.sql_database\", p_database_arguments=sql_args ) PrimalDispatcherFactory().create_dispatcher(p_ticket=ticket).start() Check the dispatcher ticket section for a complete list of parameters.","title":"Custom configuration"},{"location":"web/","text":"Web Databus has a built-in Flask website, which shows log and queue data. Whenever the dispatcher is started, the web server is activated too. The Web interface is hosted on Waitress . Web related files are stored in databus/web. It uses BootStrap , JQuery and some minor open source JavaScript libraries. Configuration If you don't do any configuration, the Web interface will start as soon as Databus is started. It runs on http://localhost:5000 by default. However, you can change the port or disable the Web interface on your dispatcher ticket . Logging in When you access the site for the first time, you are asked for three parameters to log in. Client : Name of the client you want to manage. This can be any client defined in the database , or \"root\" if you'll do something more adventureous. Username: Obvious. Password: Obvious. The web contents will change depending on the role of the user . If you log in using a client account, you can only manage that particular client. If you log in with the root account, you can manage all clients and take extra actions; such as shutting down Databus. If you just installed Databus, you can use one of the supplied accounts. demo - demo - demo root - root - root Passengers This section will list the passengers defined in the database . If you click \"Expedite\" for a passenger, the dispatcher will sync that passenger within a minute - instead of waiting for its next schedule. Peek This section can be used to peek into the source systems to see what is waiting to be pulled. For example, if your app is pulling E-Mails from Exchange, this section will peek into the inbox and show what's there. Queue This section will show the passengers in the queue along their statuses. If you click \"Purge\", the completed passengers will be removed from the queue. According to your database configuration , Databus automatically purges old queue entries anwyay - but this button is useful if you need to purge manually. If you click on a passenger , you can see its details. You can click any status to change it. For example, clicking on a \"Complete\" status will change it to \"Incomplete\"; forcing the driver to re-process it. Or; clicking on an \"Incomplete\" status will change it to \"Complete\" so that the driver won't process it further. Attachments can be viewed & downloaded directly. Log files related to that particular passenger can be viewed from here directly. Log Generated log files can be viewed here. Good for hunting down errors. If you click \"Purge\", all log files will be removed. According to your database configuration , Databus automatically purges old log files anwyay - but this button is useful if you need to purge manually. If you click on a log file, you can see its contents. Users This section will list the users defined in the database . If you click \"Revoke token\", Databus will no longer remember this user and he/she will need to re-authenticate. Customizing This is the section where you can alter system parameters. You will see two kinds of nodes here. Nodes starting with __ are files of the Databus standard. Nodes with regular names are additional files provided in the dispatcher ticket Typically, each file here is expected to be in JSON format. Some __ nodes can only be accessed with the root user. Export If you want to backup your data or migrate to another database , this is your section. Depending on the database you have selected, Databus will ask for additional parameters. The export operation can be executed in two modes: Async: In the background. Good for large volumes of data. Sync: In the foreground, real time. Good for small volumes of data. About Provides a good amount of system information. Logout The \ud83d\udc4b emoji on top can be used to logout of Databus. System The \ud83c\udf9b emoji on top can be used to access the system page. Note that this section is only available to the root user . Pause : The system will finish active drivers first, then pause safely. The dispatcher will deactivate until you resume. Resume : The system will continue working. This means that the dispatcher will activate again. Shutdown : The system will finish active drivers first, then shutdown safely. If you shutdown in any other way, you may cause unwanted data inconsistencies.","title":"Web"},{"location":"web/#web","text":"Databus has a built-in Flask website, which shows log and queue data. Whenever the dispatcher is started, the web server is activated too. The Web interface is hosted on Waitress . Web related files are stored in databus/web. It uses BootStrap , JQuery and some minor open source JavaScript libraries.","title":"Web"},{"location":"web/#configuration","text":"If you don't do any configuration, the Web interface will start as soon as Databus is started. It runs on http://localhost:5000 by default. However, you can change the port or disable the Web interface on your dispatcher ticket .","title":"Configuration"},{"location":"web/#logging-in","text":"When you access the site for the first time, you are asked for three parameters to log in. Client : Name of the client you want to manage. This can be any client defined in the database , or \"root\" if you'll do something more adventureous. Username: Obvious. Password: Obvious. The web contents will change depending on the role of the user . If you log in using a client account, you can only manage that particular client. If you log in with the root account, you can manage all clients and take extra actions; such as shutting down Databus. If you just installed Databus, you can use one of the supplied accounts. demo - demo - demo root - root - root","title":"Logging in"},{"location":"web/#passengers","text":"This section will list the passengers defined in the database . If you click \"Expedite\" for a passenger, the dispatcher will sync that passenger within a minute - instead of waiting for its next schedule.","title":"Passengers"},{"location":"web/#peek","text":"This section can be used to peek into the source systems to see what is waiting to be pulled. For example, if your app is pulling E-Mails from Exchange, this section will peek into the inbox and show what's there.","title":"Peek"},{"location":"web/#queue","text":"This section will show the passengers in the queue along their statuses. If you click \"Purge\", the completed passengers will be removed from the queue. According to your database configuration , Databus automatically purges old queue entries anwyay - but this button is useful if you need to purge manually. If you click on a passenger , you can see its details. You can click any status to change it. For example, clicking on a \"Complete\" status will change it to \"Incomplete\"; forcing the driver to re-process it. Or; clicking on an \"Incomplete\" status will change it to \"Complete\" so that the driver won't process it further. Attachments can be viewed & downloaded directly. Log files related to that particular passenger can be viewed from here directly.","title":"Queue"},{"location":"web/#log","text":"Generated log files can be viewed here. Good for hunting down errors. If you click \"Purge\", all log files will be removed. According to your database configuration , Databus automatically purges old log files anwyay - but this button is useful if you need to purge manually. If you click on a log file, you can see its contents.","title":"Log"},{"location":"web/#users","text":"This section will list the users defined in the database . If you click \"Revoke token\", Databus will no longer remember this user and he/she will need to re-authenticate.","title":"Users"},{"location":"web/#customizing","text":"This is the section where you can alter system parameters. You will see two kinds of nodes here. Nodes starting with __ are files of the Databus standard. Nodes with regular names are additional files provided in the dispatcher ticket Typically, each file here is expected to be in JSON format. Some __ nodes can only be accessed with the root user.","title":"Customizing"},{"location":"web/#export","text":"If you want to backup your data or migrate to another database , this is your section. Depending on the database you have selected, Databus will ask for additional parameters. The export operation can be executed in two modes: Async: In the background. Good for large volumes of data. Sync: In the foreground, real time. Good for small volumes of data.","title":"Export"},{"location":"web/#about","text":"Provides a good amount of system information.","title":"About"},{"location":"web/#logout","text":"The \ud83d\udc4b emoji on top can be used to logout of Databus.","title":"Logout"},{"location":"web/#system","text":"The \ud83c\udf9b emoji on top can be used to access the system page. Note that this section is only available to the root user . Pause : The system will finish active drivers first, then pause safely. The dispatcher will deactivate until you resume. Resume : The system will continue working. This means that the dispatcher will activate again. Shutdown : The system will finish active drivers first, then shutdown safely. If you shutdown in any other way, you may cause unwanted data inconsistencies.","title":"System"}]}